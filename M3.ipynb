{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "important-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "import time\n",
    "\n",
    "def split_xy(df):\n",
    "    y = df['Target']\n",
    "    x = df.drop('Target',axis=1)\n",
    "    return x,y\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('mode.chained_assignment', 'raise')SimpleImputerSimpleImputerSimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becoming-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "        \n",
    "train = pd.read_csv('../suspicious-transaction-detection/train.csv')\n",
    "# test = pd.read_csv('suspicious-transaction-detection/test.csv')\n",
    "\n",
    "N_train, dim = train.shape\n",
    "# N_test, _ = test.shape\n",
    "\n",
    "# X_train = train.drop(['Target'], axis=1)\n",
    "# y_train = train['Target'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_features = np.concatenate((['Amount'],\n",
    "                               ['T_{}'.format(t) for t in range(15)], \n",
    "                               ['C_{}'.format(c) for c in range(9, 23)],\n",
    "                               ['C_26', 'C_27', 'C_28'], \n",
    "                               ['V_{}'.format(v) for v in range(339)], \n",
    "                               ['O_0', 'O_1', 'O_5', 'O_6', 'O_9', 'O_18', 'O_31', 'O_36'],\n",
    "                               ['A_0', 'M_1']\n",
    "                              ))\n",
    "mod_features = np.concatenate((['C_0', 'C_1', 'C_2', 'C_3', 'C_4', 'C_6', 'C_7', 'C_8', 'C_23'],\n",
    "                               ['O_2', 'O_3', 'O_10', 'O_11', 'O_12', 'O_15', 'O_20', 'O_21', 'O_22', 'O_24', 'O_26', 'O_27', 'O_28', 'O_29', 'O_30', 'O_32', 'O_33', 'O_34', 'O_35', 'O_37', 'O_38'],\n",
    "                               ['A_1']\n",
    "                              ))\n",
    "oh_features = np.concatenate((['Goods', 'C_5', 'C_24', 'C_25'],\n",
    "                              ['O_4', 'O_7', 'O_8', 'browser', 'O_16', 'O_17', 'os', 'O_23', 'O_25',  'O_39'],\n",
    "                              ['E_same', 'M_0']\n",
    "                             ))\n",
    "\n",
    "print(len(med_features) + len(mod_features) + len(oh_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = pd.concat([X_train, test]).reset_index(drop=True)\n",
    "features = train\n",
    "\n",
    "features = features.drop(['TransactionID', 'Timestamp', 'O_14'], axis=1)\n",
    "\n",
    "features['browser'] = features['O_13'].str.split(' ').str[0]\n",
    "features['os'] = features['O_19'].str.split(' ').str[0]\n",
    "features['E_0'] = features['E_0'].fillna('empty')\n",
    "features['E_same'] = np.where(features['E_0'] == features['E_1'], 'T', 'F')\n",
    "\n",
    "features = features.drop(['O_13', 'O_19', 'E_0', 'E_1'], axis=1)\n",
    "\n",
    "for feature in med_features:\n",
    "    med = train[feature].median()\n",
    "    features[feature] = features[feature].fillna(med)\n",
    "for feature in mod_features:\n",
    "    mod = train[feature].mode()[0]\n",
    "    #print('{}: {}'.format(feature, mod))\n",
    "    features[feature] = features[feature].fillna(mod)\n",
    "\n",
    "features_prepared = pd.get_dummies(features, columns=oh_features)\n",
    "\n",
    "features_prepared.replace('T', 1, inplace=True)\n",
    "features_prepared.replace('F', 0, inplace=True)\n",
    "\n",
    "# xTr = \n",
    "\n",
    "# X_train_prepared = features_prepared.iloc[:N_train, :].copy()\n",
    "# X_test_prepared = features_prepared.iloc[N_train:, :].copy()\n",
    "\n",
    "# print(X_train_prepared.shape)\n",
    "# print(X_test_prepared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WORK HERE\n",
    "\n",
    "# X_train = train.drop(['Target'], axis=1)\n",
    "# y_train = train['Target'].copy()\n",
    "\n",
    "validation_data = features.sample(frac=.1)\n",
    "pre_train_data = features.drop(validation_data).reset_index(drop=True)\n",
    "xVa, yVa = split_xy(validation_data)\n",
    "\n",
    "for train_idx,test_idx in KFold(n_splits=10, random_state=1, shuffle=True).split(pre_train_data):\n",
    "    train_data = pre_train_data.iloc[train_idx]\n",
    "    test_data = pre_train_data.iloc[test_idx]\n",
    "    print(train_data)\n",
    "    \n",
    "    xTr, yTr = split_xy(train_data)\n",
    "    xTe, yTe = split_xy(test_data)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "# forest_error = cross_val_score(forest, X_train_prepared, y_train, cv=5)\n",
    "# print(forest_error.mean())\n",
    "# print(forest_error.std())\n",
    "\n",
    "forest.fit(X_train_prepared, y_train)\n",
    "y_pred = forest.predict_proba(X_test_prepared)\n",
    "roc = roc_auc_score(y_test,y_pred)\n",
    "# my_submission = pd.DataFrame({'TransactionID': test.TransactionID, 'Target': y_pred[:, 1]})\n",
    "# my_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['knn'] = {'p':[1,2,3], 'n_neighbors':[1,2,3,4,5,6,7,8,9]}\n",
    "\n",
    "grid['rf'] = {'criterion': ['gini','entropy'], 'n_estimators': [50, 100], 'max_depth':[None,2,4,6], 'min_samples_split':[2,3,4], 'min_samples_leaf':[1,2], 'max_features':[None,'auto']}\n",
    "\n",
    "grid['svmp'] = {'kernel':['poly'],'degree':[2,3], 'gamma':['scale'], 'coef0':[.01,.1,1,5,10,20,30], 'C':[.01,.1,.5,1], 'tol':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6]}\n",
    "grid['svmg'] = {'kernel':['rbf'],'gamma':['auto'],'C':[.01,.1,.5,1], 'tol':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6]}\n",
    "\n",
    "# grid['nnrelu'] = {'hidden_layer_sizes':[(10,30,10),(10,10,10)],'activation':[\"relu\"],   'learning_rate':['constant','adaptive'],'max_iter':[100,500,800],'alpha':[.1,.01,.001,.0001,.00001],'tol':[1e-2,1e-3,1e-4,1e-5],'beta_1':[.01,.1,.25,.5,.7,.9],'beta_2':[.015,.1,.25,.4,.65,.9],'epsilon':[1e-4,1e-5,1e-6,1e-7]}\n",
    "# grid['nnsig'] = {'hidden_layer_sizes':[(10,30,10),(10,10,10)],'activation':[\"logistic\"],'learning_rate':['constant','adaptive'],'max_iter':[100,500,800],'learning_rate_init':[.01,.001,.0001],'alpha':[.1,.01,.001,.0001,.00001],'tol':[1e-2,1e-3,1e-4,1e-5],'beta_1':[.01,.1,.25,.5,.7,.9],'beta_2':[.015,.1,.25,.4,.65,.9],'epsilon':[1e-4,1e-5,1e-6,1e-7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=grid['knn'], n_jobs=-1, cv=5)\n",
    "clf_knn.fit(X_train_prepared, y_train)\n",
    "preds_knn = clf_knn.predict_proba(X_test_prepared)\n",
    "\n",
    "clf_rf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=grid['rf'], n_jobs=-1, cv=5)\n",
    "clf_rf.fit(X_train_prepared, y_train)\n",
    "preds_rf = clf_rf.predict_proba(X_test_prepared)\n",
    "\n",
    "clf_svmp = GridSearchCV(estimator=svm.SVC(), param_grid=grid['svmp'], n_jobs=-1, cv=5)\n",
    "clf_svmp.fit(X_train_prepared, y_train)\n",
    "preds_svmp = clf_svmp.predict_proba(X_test_prepared)\n",
    "\n",
    "clf_svmg = GridSearchCV(estimator=svm.SVC(), param_grid=grid['svmg'], n_jobs=-1, cv=5)\n",
    "clf_svmg.fit(X_train_prepared, y_train)\n",
    "preds_svmg = clf_svmg.predict_proba(X_test_prepared)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
