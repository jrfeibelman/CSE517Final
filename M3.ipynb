{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "important-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "import time\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('mode.chained_assignment', 'raise')SimpleImputerSimpleImputerSimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becoming-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "        \n",
    "train = pd.read_csv('../suspicious-transaction-detection/train.csv')\n",
    "# test = pd.read_csv('suspicious-transaction-detection/test.csv')\n",
    "\n",
    "N_train, dim = train.shape\n",
    "# N_test, _ = test.shape\n",
    "\n",
    "# X_train = train.drop(['Target'], axis=1)\n",
    "# y_train = train['Target'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stable-treasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n"
     ]
    }
   ],
   "source": [
    "med_features = np.concatenate((['Amount'],\n",
    "                               ['T_{}'.format(t) for t in range(15)], \n",
    "                               ['C_{}'.format(c) for c in range(9, 23)],\n",
    "                               ['C_26', 'C_27', 'C_28'], \n",
    "                               ['V_{}'.format(v) for v in range(339)], \n",
    "                               ['O_0', 'O_1', 'O_5', 'O_6', 'O_9', 'O_18', 'O_31', 'O_36'],\n",
    "                               ['A_0', 'M_1']\n",
    "                              ))\n",
    "mod_features = np.concatenate((['C_0', 'C_1', 'C_2', 'C_3', 'C_4', 'C_6', 'C_7', 'C_8', 'C_23'],\n",
    "                               ['O_2', 'O_3', 'O_10', 'O_11', 'O_12', 'O_15', 'O_20', 'O_21', 'O_22', 'O_24', 'O_26', 'O_27', 'O_28', 'O_29', 'O_30', 'O_32', 'O_33', 'O_34', 'O_35', 'O_37', 'O_38'],\n",
    "                               ['A_1']\n",
    "                              ))\n",
    "oh_features = np.concatenate((['Goods', 'C_5', 'C_24', 'C_25'],\n",
    "                              ['O_4', 'O_7', 'O_8', 'browser', 'O_16', 'O_17', 'os', 'O_23', 'O_25',  'O_39'],\n",
    "                              ['E_same', 'M_0']\n",
    "                             ))\n",
    "\n",
    "print(len(med_features) + len(mod_features) + len(oh_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "permanent-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = pd.concat([X_train, test]).reset_index(drop=True)\n",
    "features = train\n",
    "\n",
    "features = features.drop(['TransactionID', 'Timestamp', 'O_14'], axis=1)\n",
    "\n",
    "features['browser'] = features['O_13'].str.split(' ').str[0]\n",
    "features['os'] = features['O_19'].str.split(' ').str[0]\n",
    "features['E_0'] = features['E_0'].fillna('empty')\n",
    "features['E_same'] = np.where(features['E_0'] == features['E_1'], 'T', 'F')\n",
    "\n",
    "features = features.drop(['O_13', 'O_19', 'E_0', 'E_1'], axis=1)\n",
    "\n",
    "for feature in med_features:\n",
    "    med = train[feature].median()\n",
    "    features[feature] = features[feature].fillna(med)\n",
    "for feature in mod_features:\n",
    "    mod = train[feature].mode()[0]\n",
    "    #print('{}: {}'.format(feature, mod))\n",
    "    features[feature] = features[feature].fillna(mod)\n",
    "\n",
    "features_prepared = pd.get_dummies(features, columns=oh_features)\n",
    "\n",
    "features_prepared.replace('T', 1, inplace=True)\n",
    "features_prepared.replace('F', 0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# xTr = \n",
    "\n",
    "# X_train_prepared = features_prepared.iloc[:N_train, :].copy()\n",
    "# X_test_prepared = features_prepared.iloc[N_train:, :].copy()\n",
    "\n",
    "# print(X_train_prepared.shape)\n",
    "# print(X_test_prepared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "editorial-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "august-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def split_xy(df):\n",
    "    y = df['Target']\n",
    "    x = df.drop('Target',axis=1)\n",
    "    return x,y\n",
    "\n",
    "# x,y = split_xy(features_prepared)\n",
    "# c = x.columns\n",
    "# standard = preprocessing.StandardScaler()\n",
    "# x = pd.DataFrame(standard.fit_transform(x),columns=c)\n",
    "# x\n",
    "\n",
    "# max_scaler = preprocessing.MaxAbsScaler()\n",
    "# standard = preprocessing.StandardScaler()\n",
    "# x = standard.fit_transform(features_prepared)\n",
    "# X_train_max = max_scaler.fit_transform(X_train_standard)\n",
    "\n",
    "# pca = PCA(n_components=3)\n",
    "# pc = pd.DataFrame(pca.fit_transform(x))\n",
    "# pc['Target'] = y\n",
    "# print(pc)\n",
    "validation_data = features_prepared.sample(frac=.1)\n",
    "pre_train_data = features_prepared.drop(validation_data.index).reset_index(drop=True)\n",
    "xTr, yTr = split_xy(pre_train_data)\n",
    "xTe, yTe = split_xy(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-flour",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "atomic-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "impressive-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['knn'] = {'p':[1,2], 'n_neighbors':[1,2,3,4,5,6,7,8,9]}\n",
    "\n",
    "grid['rf'] = {'n_estimators': [10,50], 'max_depth':[None,2,4,6]}\n",
    "\n",
    "# grid['svmp'] = {'kernel':['poly'],'degree':[2,3], 'gamma':['scale'], 'coef0':[.01,.1,1,5,10,20], 'C':[.01,.1,.5,1], 'tol':[1e-1,1e-2,1e-3,1e-4]}\n",
    "grid['svmg'] = {'kernel':['rbf'],'gamma':['auto'],'C':[.01,.1,.5,1], 'tol':[1e-1,1e-2,1e-3,1e-4]}\n",
    "\n",
    "# grid['nnrelu'] = {'hidden_layer_sizes':[(10,30,10),(10,10,10)],'activation':[\"relu\"],   'learning_rate':['constant','adaptive'],'max_iter':[100,500,800],'alpha':[.1,.01,.001,.0001,.00001],'tol':[1e-2,1e-3,1e-4,1e-5],'beta_1':[.01,.1,.25,.5,.7,.9],'beta_2':[.015,.1,.25,.4,.65,.9],'epsilon':[1e-4,1e-5,1e-6,1e-7]}\n",
    "# grid['nnsig'] = {'hidden_layer_sizes':[(10,30,10),(10,10,10)],'activation':[\"logistic\"],'learning_rate':['constant','adaptive'],'max_iter':[100,500,800],'learning_rate_init':[.01,.001,.0001],'alpha':[.1,.01,.001,.0001,.00001],'tol':[1e-2,1e-3,1e-4,1e-5],'beta_1':[.01,.1,.25,.5,.7,.9],'beta_2':[.015,.1,.25,.4,.65,.9],'epsilon':[1e-4,1e-5,1e-6,1e-7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, random_state=1, shuffle=True).split(pre_train_data)\n",
    "\n",
    "clf_knn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=grid['knn'], n_jobs=-1, cv=cv)\n",
    "clf_knn.fit(xTr, yTr)\n",
    "preds_knn = clf_knn.predict_proba(xTe)\n",
    "roc_knn = roc_auc_score(yTe,preds_knn)\n",
    "print(\"KNN AUC : %s - %s\" % (roc_knn,clf_knn.best_params_))\n",
    "\n",
    "clf_rf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=grid['rf'], n_jobs=-1, cv=cv)\n",
    "clf_rf.fit(xTr, yTr)\n",
    "preds_rf = clf_rf.predict_proba(xTe)\n",
    "roc_rf = roc_auc_score(yTe,preds_rf)\n",
    "print(\"RF AUC : %s - %s\" % (roc_rf,clf_rf.best_params_))\n",
    "\n",
    "# clf_svmp = GridSearchCV(estimator=svm.SVC(), param_grid=grid['svmp'], n_jobs=-1, cv=cv)\n",
    "# clf_svmp.fit(xTr, yTr)\n",
    "# preds_svmp = clf_svmp.predict_proba(xTe)\n",
    "# roc_svmp = roc_auc_score(yTe,preds_svmp)\n",
    "# print(\"SVM[P] AUC : %s - %s\" % (roc_svmp,clf_svmp.best_params_))\n",
    "\n",
    "\n",
    "standard = preprocessing.StandardScaler()\n",
    "xTr_svm = standard.fit_transform(xTr)\n",
    "xTe_svm = standard.fit_transform(xTe)\n",
    "\n",
    "clf_svmg = GridSearchCV(estimator=svm.SVC(), param_grid=grid['svmg'], n_jobs=-1, cv=cv)\n",
    "clf_svmg.fit(xTr_svm, yTr)\n",
    "preds_svmg = clf_svmg.predict_proba(xTe_svm)\n",
    "roc_svmg = roc_auc_score(yTe,preds_svmg)\n",
    "print(\"SVM[G] AUC : %s - %s\" % (roc_svmg,clf_svmg.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-kitty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
